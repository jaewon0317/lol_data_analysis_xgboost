{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 데이터 전처리 및 Feature 선택",
   "id": "49f7852bdb9a4bdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:23:45.897720Z",
     "start_time": "2025-12-06T18:05:45.213435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Timeline 데이터에서 Feature 추출\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.position_mapping = {}  # matchId -> {participantId: position}\n",
    "        self.team_mapping = {}  # matchId -> {participantId: teamId}\n",
    "        self.puuid_mapping = {}  # matchId -> {participantId: puuid}\n",
    "        self.win_mapping = {}  # matchId -> {participantId: win}\n",
    "\n",
    "    def load_match_metadata(self, match_data: List[Dict]) -> None:\n",
    "        \"\"\"Match 데이터에서 포지션, 팀, puuid, 승패 정보 로드\"\"\"\n",
    "        for match in match_data:\n",
    "            match_id = match['metadata']['matchId']\n",
    "            self.position_mapping[match_id] = {}\n",
    "            self.team_mapping[match_id] = {}\n",
    "            self.puuid_mapping[match_id] = {}\n",
    "            self.win_mapping[match_id] = {}\n",
    "\n",
    "            for p in match['info']['participants']:\n",
    "                pid = p['participantId']\n",
    "                self.position_mapping[match_id][pid] = p.get('teamPosition', '')\n",
    "                self.team_mapping[match_id][pid] = p.get('teamId')\n",
    "                self.puuid_mapping[match_id][pid] = p.get('puuid')\n",
    "                self.win_mapping[match_id][pid] = p.get('win')\n",
    "\n",
    "    # [수정됨] 특정 포지션에 맞는 상대방 ID를 찾도록 일반화\n",
    "    def get_opponent_id(self, match_id: str, my_participant_id: int, my_position: str) -> int:\n",
    "        my_team = self.team_mapping[match_id][my_participant_id]\n",
    "        for pid, pos in self.position_mapping[match_id].items():\n",
    "            # 나와 포지션이 같고, 팀이 다른 경우를 상대방으로 간주\n",
    "            if pos == my_position and self.team_mapping[match_id][pid] != my_team:\n",
    "                return pid\n",
    "        return -1\n",
    "\n",
    "    def get_team_participant_ids(self, match_id: str, participant_id: int) -> List[int]:\n",
    "        my_team = self.team_mapping[match_id][participant_id]\n",
    "        return [pid for pid, team in self.team_mapping[match_id].items() if team == my_team]\n",
    "\n",
    "    def parse_events_cumulative(self, frames: List[Dict], until_frame: int, participant_id: int) -> Dict:\n",
    "        result = {\n",
    "            'kills': 0, 'deaths': 0, 'assists': 0, 'soloKills': 0,\n",
    "            'turretPlates': 0, 'wardsPlaced': 0, 'objectiveParticipation': 0\n",
    "        }\n",
    "\n",
    "        for i, frame in enumerate(frames[:until_frame + 1]):\n",
    "            for event in frame.get('events', []):\n",
    "                event_type = event.get('type')\n",
    "\n",
    "                if event_type == 'CHAMPION_KILL':\n",
    "                    killer_id = event.get('killerId')\n",
    "                    victim_id = event.get('victimId')\n",
    "                    assisting = event.get('assistingParticipantIds', [])\n",
    "\n",
    "                    if killer_id == participant_id:\n",
    "                        result['kills'] += 1\n",
    "                        if not assisting:\n",
    "                            result['soloKills'] += 1\n",
    "                    if victim_id == participant_id:\n",
    "                        result['deaths'] += 1\n",
    "                    if participant_id in assisting:\n",
    "                        result['assists'] += 1\n",
    "\n",
    "                elif event_type == 'TURRET_PLATE_DESTROYED':\n",
    "                    if event.get('killerId') == participant_id:\n",
    "                        result['turretPlates'] += 1\n",
    "\n",
    "                elif event_type == 'WARD_PLACED':\n",
    "                    if event.get('creatorId') == participant_id:\n",
    "                        result['wardsPlaced'] += 1\n",
    "\n",
    "                elif event_type == 'ELITE_MONSTER_KILL':\n",
    "                    if event.get('killerId') == participant_id:\n",
    "                        result['objectiveParticipation'] += 1\n",
    "                    elif participant_id in event.get('assistingParticipantIds', []):\n",
    "                        result['objectiveParticipation'] += 1\n",
    "        return result\n",
    "\n",
    "    def get_team_stats_at_frame(self, frame: Dict, team_pids: List[int]) -> Dict:\n",
    "        team_damage_to_champs = 0\n",
    "        team_damage_taken = 0\n",
    "        for pid in team_pids:\n",
    "            pf = frame['participantFrames'].get(str(pid), {})\n",
    "            ds = pf.get('damageStats', {})\n",
    "            team_damage_to_champs += ds.get('totalDamageDoneToChampions', 0)\n",
    "            team_damage_taken += ds.get('totalDamageTaken', 0)\n",
    "        return {'teamDamageToChamps': team_damage_to_champs, 'teamDamageTaken': team_damage_taken}\n",
    "\n",
    "    def count_team_kills(self, frames: List[Dict], until_frame: int, team_pids: List[int]) -> int:\n",
    "        team_kills = 0\n",
    "        for i, frame in enumerate(frames[:until_frame + 1]):\n",
    "            for event in frame.get('events', []):\n",
    "                if event.get('type') == 'CHAMPION_KILL':\n",
    "                    if event.get('killerId') in team_pids:\n",
    "                        team_kills += 1\n",
    "        return team_kills\n",
    "\n",
    "    def extract_features_for_participant(self, match_id: str, frames: List[Dict], participant_id: int, position: str) -> \\\n",
    "    List[Dict]:\n",
    "        # [수정됨] 포지션에 상관없이 해당 포지션의 상대방 ID를 가져옴\n",
    "        opponent_id = self.get_opponent_id(match_id, participant_id, position)\n",
    "        team_pids = self.get_team_participant_ids(match_id, participant_id)\n",
    "\n",
    "        records = []\n",
    "        for frame_idx, frame in enumerate(frames):\n",
    "            pf = frame['participantFrames'].get(str(participant_id))\n",
    "            if not pf: continue\n",
    "\n",
    "            # 상대방 데이터가 있으면 가져오고, 없으면(상대 라이너 없음 등) 빈 값 처리\n",
    "            opp_pf = frame['participantFrames'].get(str(opponent_id), {}) if opponent_id > 0 else {}\n",
    "            opp_ds = opp_pf.get('damageStats', {})\n",
    "            event_features = self.parse_events_cumulative(frames, frame_idx, participant_id)\n",
    "            opp_event_features = self.parse_events_cumulative(frames, frame_idx, opponent_id) if opponent_id > 0 else {\n",
    "                'kills': 0, 'deaths': 0, 'assists': 0, 'soloKills': 0,\n",
    "                'turretPlates': 0, 'wardsPlaced': 0, 'objectiveParticipation': 0\n",
    "            }\n",
    "            team_stats = self.get_team_stats_at_frame(frame, team_pids)\n",
    "            team_kills = self.count_team_kills(frames, frame_idx, team_pids)\n",
    "            ds = pf.get('damageStats', {})\n",
    "\n",
    "            xp = pf.get('xp', 0)\n",
    "            level = pf.get('level', 1)\n",
    "            minions_killed = pf.get('minionsKilled', 0)\n",
    "            jungle_minions_killed = pf.get('jungleMinionsKilled', 0)\n",
    "            total_damage_to_champs = ds.get('totalDamageDoneToChampions', 0)\n",
    "            total_damage_taken = ds.get('totalDamageTaken', 0)\n",
    "            time_enemy_controlled = pf.get('timeEnemySpentControlled', 0)\n",
    "\n",
    "            opp_xp = opp_pf.get('xp', 0)\n",
    "            opp_level = opp_pf.get('level', 1)\n",
    "            opp_minions = opp_pf.get('minionsKilled', 0) + opp_pf.get('jungleMinionsKilled', 0)\n",
    "            opp_damage_to_champs = opp_ds.get('totalDamageDoneToChampions', 0)\n",
    "\n",
    "            total_cs = minions_killed + jungle_minions_killed\n",
    "            kills = event_features['kills']\n",
    "            deaths = event_features['deaths']\n",
    "            assists = event_features['assists']\n",
    "\n",
    "            kda = (kills + assists) / max(deaths, 1)\n",
    "            kill_participation = (kills + assists) / max(team_kills, 1)\n",
    "            damage_share = total_damage_to_champs / max(team_stats['teamDamageToChamps'], 1)\n",
    "\n",
    "            xp_diff = xp - opp_xp\n",
    "            cs_diff = total_cs - opp_minions\n",
    "            damage_diff = total_damage_to_champs - opp_damage_to_champs\n",
    "            kills_diff = kills - opp_event_features['kills']\n",
    "            level_diff = level - opp_level\n",
    "\n",
    "            total_gold = pf.get('totalGold', 0)\n",
    "\n",
    "            record = {\n",
    "                'matchId': match_id,\n",
    "                'participantId': participant_id,\n",
    "                'puuid': self.puuid_mapping.get(match_id, {}).get(participant_id, ''),\n",
    "                'position': position,\n",
    "                'minute': frame_idx,\n",
    "                'win': self.win_mapping.get(match_id, {}).get(participant_id),\n",
    "                'xp': xp, 'level': level, 'minionsKilled': minions_killed,\n",
    "                'jungleMinionsKilled': jungle_minions_killed,\n",
    "                'totalDamageDoneToChampions': total_damage_to_champs,\n",
    "                'totalDamageTaken': total_damage_taken,\n",
    "                'timeEnemySpentControlled': time_enemy_controlled,\n",
    "                'kills': kills, 'deaths': deaths, 'assists': assists,\n",
    "                'soloKills': event_features['soloKills'],\n",
    "                'turretPlates': event_features['turretPlates'],\n",
    "                'wardsPlaced': event_features['wardsPlaced'],\n",
    "                'objectiveParticipation': event_features['objectiveParticipation'],\n",
    "                'xpDiff': xp_diff, 'csDiff': cs_diff, 'damageDiff': damage_diff,\n",
    "                'killsDiff': kills_diff, 'levelDiff': level_diff,\n",
    "                'KDA': kda, 'killParticipation': kill_participation, 'damageShare': damage_share,\n",
    "                'totalGold': total_gold\n",
    "            }\n",
    "            records.append(record)\n",
    "        return records\n",
    "\n",
    "    def extract_all_top_features(self, timeline_data: List[Dict], match_data: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"배치 단위 처리를 위해 로직 유지 (이름은 기존 호환성을 위해 유지하되 내용은 전체 포지션 처리)\"\"\"\n",
    "        self.load_match_metadata(match_data)\n",
    "        timeline_by_match = {t['metadata']['matchId']: t for t in timeline_data}\n",
    "\n",
    "        # [수정됨] 추출할 대상 포지션 목록 정의\n",
    "        target_positions = {'TOP', 'JUNGLE', 'MIDDLE', 'BOTTOM', 'UTILITY'}\n",
    "\n",
    "        all_records = []\n",
    "        for match in match_data:\n",
    "            match_id = match['metadata']['matchId']\n",
    "            if match_id not in timeline_by_match: continue\n",
    "\n",
    "            timeline = timeline_by_match[match_id]\n",
    "            frames = timeline['info']['frames']\n",
    "\n",
    "            for pid, pos in self.position_mapping[match_id].items():\n",
    "                # [수정됨] TOP뿐만 아니라 target_positions에 포함된 모든 포지션에 대해 추출\n",
    "                if pos in target_positions:\n",
    "                    records = self.extract_features_for_participant(match_id, frames, pid, pos)\n",
    "                    all_records.extend(records)\n",
    "        return pd.DataFrame(all_records)\n",
    "\n",
    "\n",
    "# --- [유틸리티 함수] ---\n",
    "\n",
    "def load_json_file_single(path: str) -> Dict:\n",
    "    \"\"\"단일 JSON 파일 로드\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def discover_data_files(data_dir: str = 'match_data') -> Tuple[List[str], List[str]]:\n",
    "    data_path = Path(data_dir)\n",
    "    match_files = sorted(data_path.glob('match_*.json'))\n",
    "    timeline_files = sorted(data_path.glob('timeline_*.json'))\n",
    "    return [str(f) for f in match_files], [str(f) for f in timeline_files]\n",
    "\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'xp', 'level', 'minionsKilled', 'jungleMinionsKilled',\n",
    "    'totalDamageDoneToChampions', 'totalDamageTaken', 'timeEnemySpentControlled',\n",
    "    'kills', 'deaths', 'assists', 'soloKills',\n",
    "    'turretPlates', 'wardsPlaced', 'objectiveParticipation',\n",
    "    'xpDiff', 'csDiff', 'damageDiff', 'killsDiff', 'levelDiff',\n",
    "    'KDA', 'killParticipation', 'damageShare'\n",
    "]\n",
    "TARGET_COLUMN = 'totalGold'\n",
    "META_COLUMNS = ['matchId', 'participantId', 'puuid', 'position', 'minute', 'win']\n",
    "\n",
    "# --- [Main 실행 부분] ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Discovering data files in match_data directory...\")\n",
    "    match_files, timeline_files = discover_data_files('data/match_data')\n",
    "\n",
    "    total_files = len(match_files)\n",
    "    print(f\"Found {total_files} match/timeline file pairs.\")\n",
    "\n",
    "    if total_files == 0:\n",
    "        print(\"Error: No data files found.\")\n",
    "        exit(1)\n",
    "\n",
    "    extractor = FeatureExtractor()\n",
    "    all_dfs = []  # 배치별 결과를 모을 리스트\n",
    "\n",
    "    # 배치 사이즈 설정\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "    print(\"\\nStarting batch processing...\")\n",
    "\n",
    "    for i in range(0, total_files, BATCH_SIZE):\n",
    "        batch_match_paths = match_files[i: i + BATCH_SIZE]\n",
    "        batch_timeline_paths = timeline_files[i: i + BATCH_SIZE]\n",
    "\n",
    "        batch_match_data = [load_json_file_single(p) for p in batch_match_paths]\n",
    "        batch_timeline_data = [load_json_file_single(p) for p in batch_timeline_paths]\n",
    "\n",
    "        flat_match_data = []\n",
    "        for d in batch_match_data:\n",
    "            if isinstance(d, list):\n",
    "                flat_match_data.extend(d)\n",
    "            else:\n",
    "                flat_match_data.append(d)\n",
    "\n",
    "        flat_timeline_data = []\n",
    "        for d in batch_timeline_data:\n",
    "            if isinstance(d, list):\n",
    "                flat_timeline_data.extend(d)\n",
    "            else:\n",
    "                flat_timeline_data.append(d)\n",
    "\n",
    "        batch_df = extractor.extract_all_top_features(flat_timeline_data, flat_match_data)\n",
    "        all_dfs.append(batch_df)\n",
    "\n",
    "        current_count = min(i + BATCH_SIZE, total_files)\n",
    "        print(f\"✅ Processed {current_count}/{total_files} files ({(current_count / total_files) * 100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nConcatenating all batches...\")\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nExtracted {len(final_df)} rows total.\")\n",
    "\n",
    "    if not final_df.empty:\n",
    "        ratio_columns = ['KDA', 'killParticipation', 'damageShare']\n",
    "        for col in ratio_columns:\n",
    "            if col in final_df.columns:\n",
    "                final_df[col] = final_df[col].round(4)\n",
    "\n",
    "        # [수정됨] 출력 파일명을 전체 포지션에 맞게 변경\n",
    "        output_path = 'data/all_positions_features.csv'\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSaved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No features extracted.\")"
   ],
   "id": "40be23d63c080314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering data files in match_data directory...\n",
      "Found 1075 match/timeline file pairs.\n",
      "\n",
      "Starting batch processing...\n",
      "✅ Processed 10/1075 files (0.9%)\n",
      "✅ Processed 20/1075 files (1.9%)\n",
      "✅ Processed 30/1075 files (2.8%)\n",
      "✅ Processed 40/1075 files (3.7%)\n",
      "✅ Processed 50/1075 files (4.7%)\n",
      "✅ Processed 60/1075 files (5.6%)\n",
      "✅ Processed 70/1075 files (6.5%)\n",
      "✅ Processed 80/1075 files (7.4%)\n",
      "✅ Processed 90/1075 files (8.4%)\n",
      "✅ Processed 100/1075 files (9.3%)\n",
      "✅ Processed 110/1075 files (10.2%)\n",
      "✅ Processed 120/1075 files (11.2%)\n",
      "✅ Processed 130/1075 files (12.1%)\n",
      "✅ Processed 140/1075 files (13.0%)\n",
      "✅ Processed 150/1075 files (14.0%)\n",
      "✅ Processed 160/1075 files (14.9%)\n",
      "✅ Processed 170/1075 files (15.8%)\n",
      "✅ Processed 180/1075 files (16.7%)\n",
      "✅ Processed 190/1075 files (17.7%)\n",
      "✅ Processed 200/1075 files (18.6%)\n",
      "✅ Processed 210/1075 files (19.5%)\n",
      "✅ Processed 220/1075 files (20.5%)\n",
      "✅ Processed 230/1075 files (21.4%)\n",
      "✅ Processed 240/1075 files (22.3%)\n",
      "✅ Processed 250/1075 files (23.3%)\n",
      "✅ Processed 260/1075 files (24.2%)\n",
      "✅ Processed 270/1075 files (25.1%)\n",
      "✅ Processed 280/1075 files (26.0%)\n",
      "✅ Processed 290/1075 files (27.0%)\n",
      "✅ Processed 300/1075 files (27.9%)\n",
      "✅ Processed 310/1075 files (28.8%)\n",
      "✅ Processed 320/1075 files (29.8%)\n",
      "✅ Processed 330/1075 files (30.7%)\n",
      "✅ Processed 340/1075 files (31.6%)\n",
      "✅ Processed 350/1075 files (32.6%)\n",
      "✅ Processed 360/1075 files (33.5%)\n",
      "✅ Processed 370/1075 files (34.4%)\n",
      "✅ Processed 380/1075 files (35.3%)\n",
      "✅ Processed 390/1075 files (36.3%)\n",
      "✅ Processed 400/1075 files (37.2%)\n",
      "✅ Processed 410/1075 files (38.1%)\n",
      "✅ Processed 420/1075 files (39.1%)\n",
      "✅ Processed 430/1075 files (40.0%)\n",
      "✅ Processed 440/1075 files (40.9%)\n",
      "✅ Processed 450/1075 files (41.9%)\n",
      "✅ Processed 460/1075 files (42.8%)\n",
      "✅ Processed 470/1075 files (43.7%)\n",
      "✅ Processed 480/1075 files (44.7%)\n",
      "✅ Processed 490/1075 files (45.6%)\n",
      "✅ Processed 500/1075 files (46.5%)\n",
      "✅ Processed 510/1075 files (47.4%)\n",
      "✅ Processed 520/1075 files (48.4%)\n",
      "✅ Processed 530/1075 files (49.3%)\n",
      "✅ Processed 540/1075 files (50.2%)\n",
      "✅ Processed 550/1075 files (51.2%)\n",
      "✅ Processed 560/1075 files (52.1%)\n",
      "✅ Processed 570/1075 files (53.0%)\n",
      "✅ Processed 580/1075 files (54.0%)\n",
      "✅ Processed 590/1075 files (54.9%)\n",
      "✅ Processed 600/1075 files (55.8%)\n",
      "✅ Processed 610/1075 files (56.7%)\n",
      "✅ Processed 620/1075 files (57.7%)\n",
      "✅ Processed 630/1075 files (58.6%)\n",
      "✅ Processed 640/1075 files (59.5%)\n",
      "✅ Processed 650/1075 files (60.5%)\n",
      "✅ Processed 660/1075 files (61.4%)\n",
      "✅ Processed 670/1075 files (62.3%)\n",
      "✅ Processed 680/1075 files (63.3%)\n",
      "✅ Processed 690/1075 files (64.2%)\n",
      "✅ Processed 700/1075 files (65.1%)\n",
      "✅ Processed 710/1075 files (66.0%)\n",
      "✅ Processed 720/1075 files (67.0%)\n",
      "✅ Processed 730/1075 files (67.9%)\n",
      "✅ Processed 740/1075 files (68.8%)\n",
      "✅ Processed 750/1075 files (69.8%)\n",
      "✅ Processed 760/1075 files (70.7%)\n",
      "✅ Processed 770/1075 files (71.6%)\n",
      "✅ Processed 780/1075 files (72.6%)\n",
      "✅ Processed 790/1075 files (73.5%)\n",
      "✅ Processed 800/1075 files (74.4%)\n",
      "✅ Processed 810/1075 files (75.3%)\n",
      "✅ Processed 820/1075 files (76.3%)\n",
      "✅ Processed 830/1075 files (77.2%)\n",
      "✅ Processed 840/1075 files (78.1%)\n",
      "✅ Processed 850/1075 files (79.1%)\n",
      "✅ Processed 860/1075 files (80.0%)\n",
      "✅ Processed 870/1075 files (80.9%)\n",
      "✅ Processed 880/1075 files (81.9%)\n",
      "✅ Processed 890/1075 files (82.8%)\n",
      "✅ Processed 900/1075 files (83.7%)\n",
      "✅ Processed 910/1075 files (84.7%)\n",
      "✅ Processed 920/1075 files (85.6%)\n",
      "✅ Processed 930/1075 files (86.5%)\n",
      "✅ Processed 940/1075 files (87.4%)\n",
      "✅ Processed 950/1075 files (88.4%)\n",
      "✅ Processed 960/1075 files (89.3%)\n",
      "✅ Processed 970/1075 files (90.2%)\n",
      "✅ Processed 980/1075 files (91.2%)\n",
      "✅ Processed 990/1075 files (92.1%)\n",
      "✅ Processed 1000/1075 files (93.0%)\n",
      "✅ Processed 1010/1075 files (94.0%)\n",
      "✅ Processed 1020/1075 files (94.9%)\n",
      "✅ Processed 1030/1075 files (95.8%)\n",
      "✅ Processed 1040/1075 files (96.7%)\n",
      "✅ Processed 1050/1075 files (97.7%)\n",
      "✅ Processed 1060/1075 files (98.6%)\n",
      "✅ Processed 1070/1075 files (99.5%)\n",
      "✅ Processed 1075/1075 files (100.0%)\n",
      "\n",
      "Concatenating all batches...\n",
      "\n",
      "Extracted 11270680 rows total.\n",
      "\n",
      "Saved to data/all_positions_features.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 봇 데이터 병합 및 최종 저장",
   "id": "488889181331b41e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T19:47:05.345295Z",
     "start_time": "2025-12-06T19:46:10.163220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_bot_duo_and_save(input_path, output_path):\n",
    "    print(f\"Loading raw data from {input_path}...\")\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # 1. 바텀 듀오(BOTTOM, UTILITY)와 솔로 라이너(TOP, JUNGLE, MIDDLE) 분리\n",
    "    duo_df = df[df['position'].isin(['BOTTOM', 'UTILITY'])].copy()\n",
    "    solo_df = df[~df['position'].isin(['BOTTOM', 'UTILITY'])].copy()\n",
    "\n",
    "    print(\"Merging BOTTOM and UTILITY...\")\n",
    "\n",
    "    # 2. 바텀 듀오 합치기 (MatchId, Minute, TeamId 등이 같은 그룹끼리)\n",
    "    # 주의: 문자열 데이터(win, puuid 등)와 숫자 데이터를 구분해서 처리\n",
    "\n",
    "    # 2-1. 단순 합산할 컬럼들 (수치형)\n",
    "    sum_cols = [\n",
    "        'kills', 'deaths', 'assists', 'minionsKilled', 'jungleMinionsKilled',\n",
    "        'totalDamageDoneToChampions', 'totalDamageTaken', 'timeEnemySpentControlled',\n",
    "        'wardsPlaced', 'turretPlates', 'objectiveParticipation', 'totalGold',\n",
    "        'xpDiff', 'csDiff', 'damageDiff', 'killsDiff', 'levelDiff' # 격차도 합산하면 '듀오 간 격차'가 됨\n",
    "    ]\n",
    "\n",
    "    # 2-2. 평균을 낼 컬럼들 (선택 사항, 레벨 등)\n",
    "    mean_cols = ['xp', 'level']\n",
    "\n",
    "    # 그룹화 기준 (경기, 시간, 승패는 동일하므로 포함)\n",
    "    group_keys = ['matchId', 'minute', 'win']\n",
    "\n",
    "    # 합산 실행\n",
    "    duo_sum = duo_df.groupby(group_keys)[sum_cols].sum().reset_index()\n",
    "    duo_mean = duo_df.groupby(group_keys)[mean_cols].mean().reset_index()\n",
    "\n",
    "    # 데이터 병합\n",
    "    merged_duo = pd.merge(duo_sum, duo_mean, on=group_keys)\n",
    "\n",
    "    # 3. 파생 변수 재계산 (비율 데이터는 합치면 왜곡되므로 다시 구함)\n",
    "    # KDA 재계산\n",
    "    merged_duo['KDA'] = (merged_duo['kills'] + merged_duo['assists']) / merged_duo['deaths'].replace(0, 1)\n",
    "    # 포지션 이름 지정\n",
    "    merged_duo['position'] = 'BOT_DUO'\n",
    "    # PUUID는 두 명이라 합치기 애매하므로 'BOT_DUO'로 대체하거나 이어붙임\n",
    "    merged_duo['puuid'] = 'DUO_PLAYER'\n",
    "\n",
    "    # (선택) 킬 관여율 등은 팀 전체 데이터가 필요해서 여기선 생략하거나 근사치 사용\n",
    "    # 기존 컬럼 형식을 맞추기 위해 빈 컬럼 채우기 (필요하다면)\n",
    "    merged_duo['killParticipation'] = 0 # 필요시 로직 추가\n",
    "    merged_duo['damageShare'] = 0       # 필요시 로직 추가\n",
    "\n",
    "    # 4. 솔로 라이너와 다시 합치기\n",
    "    # 컬럼 순서 맞추기\n",
    "    final_df = pd.concat([solo_df, merged_duo], ignore_index=True)\n",
    "\n",
    "    # 5. 정렬 (경기 -> 시간 -> 포지션 순)\n",
    "    final_df = final_df.sort_values(['matchId', 'minute', 'position'])\n",
    "\n",
    "    # 저장\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Merged {len(df)} rows into {len(final_df)} rows.\")\n",
    "    print(f\"✅ Saved to {output_path}\")\n",
    "    print(\"Now you have positions: \", final_df['position'].unique())\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 01번 파일에서 저장한 경로\n",
    "    INPUT_FILE = 'data/all_positions_features.csv'\n",
    "    # 02~06번 파일에서 사용할 경로\n",
    "    OUTPUT_FILE = 'data/final_features.csv'\n",
    "\n",
    "    merge_bot_duo_and_save(INPUT_FILE, OUTPUT_FILE)"
   ],
   "id": "efe0341da03a799e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from data/all_positions_features.csv...\n",
      "Merging BOTTOM and UTILITY...\n",
      "✅ Merged 11270680 rows into 9016544 rows.\n",
      "✅ Saved to data/final_features_ready.csv\n",
      "Now you have positions:  ['BOT_DUO' 'JUNGLE' 'MIDDLE' 'TOP']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b818bb56b82535f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
